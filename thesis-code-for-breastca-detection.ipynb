{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Catherine de Andres Arceno\n# Breast Cancer detection using Ensemble of homogeneous and heterogeneous Transfer learning methods\n\n## Approach to complete problem statement\n- **Handling Imbalance distribution -** First after loading data we will handle Imbalance case\n- **Data Augmentation -** We will augment the data for better predictions\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"#import libraries\nimport pandas as pd\nimport numpy as np\nimport keras\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport glob\n\nfrom keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.keras.utils import to_categorical\nimport cv2\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2021-08-29T20:57:36.654150Z","iopub.execute_input":"2021-08-29T20:57:36.654469Z","iopub.status.idle":"2021-08-29T20:57:36.660469Z","shell.execute_reply.started":"2021-08-29T20:57:36.654440Z","shell.execute_reply":"2021-08-29T20:57:36.659261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#loading files\nfiles = glob.glob('/kaggle/input/breast-histopathology-images/*/*/*')","metadata":{"execution":{"iopub.status.busy":"2021-08-29T20:57:45.080204Z","iopub.execute_input":"2021-08-29T20:57:45.080519Z","iopub.status.idle":"2021-08-29T20:57:46.433736Z","shell.execute_reply.started":"2021-08-29T20:57:45.080488Z","shell.execute_reply":"2021-08-29T20:57:46.432889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_img(files):\n    plt.figure(figsize= (10,10))\n    index = np.random.randint(0, len(files), 25)\n    i=0\n    for loc in index:\n        plt.subplot(5,5,i+1)\n        sample = load_img(files[loc], target_size=(150,150))\n        sample = img_to_array(sample)\n        plt.axis(\"off\")\n        plt.imshow(sample.astype(\"uint8\"))\n        i+=1\n        \nshow_img(files)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T20:57:54.813606Z","iopub.execute_input":"2021-08-29T20:57:54.813965Z","iopub.status.idle":"2021-08-29T20:57:56.228557Z","shell.execute_reply.started":"2021-08-29T20:57:54.813934Z","shell.execute_reply":"2021-08-29T20:57:56.227603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Load Data**","metadata":{}},{"cell_type":"code","source":"\ndef load_data(files, lower_limit, upper_limit):\n    data = []\n    labels = []\n    for file in files[lower_limit : upper_limit]:\n        if file.endswith(\".png\"):\n            img = load_img(file, target_size=(50,50)) \n            pixels = img_to_array(img)\n            pixels /= 255\n            data.append(pixels)\n            if(file[-5] == \"1\"):\n                labels.append(1)\n            elif(file[-5] == \"0\"):\n                labels.append(0)\n                \n    return np.stack(data), labels\n    ","metadata":{"execution":{"iopub.status.busy":"2021-08-29T20:58:32.787854Z","iopub.execute_input":"2021-08-29T20:58:32.788189Z","iopub.status.idle":"2021-08-29T20:58:32.796090Z","shell.execute_reply.started":"2021-08-29T20:58:32.788159Z","shell.execute_reply":"2021-08-29T20:58:32.794975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n#loading 90000 imgs of each cls\n#x_train, y_train = load_data(files, 0, 90000)\n#20000 imgs for testing\n#x_test, y_test = load_data(files, 90000, 110000)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T20:58:40.524581Z","iopub.execute_input":"2021-08-29T20:58:40.524987Z","iopub.status.idle":"2021-08-29T20:58:40.529816Z","shell.execute_reply.started":"2021-08-29T20:58:40.524954Z","shell.execute_reply":"2021-08-29T20:58:40.528883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets visualize the distribution of data in both classes\n#sns.countplot(y_train)\n#plt.title(\"class distribution in trainin data\")\n#plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-29T20:58:50.137083Z","iopub.execute_input":"2021-08-29T20:58:50.137435Z","iopub.status.idle":"2021-08-29T20:58:50.142986Z","shell.execute_reply.started":"2021-08-29T20:58:50.137404Z","shell.execute_reply":"2021-08-29T20:58:50.142169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Handling Imbalance Distribution of data","metadata":{}},{"cell_type":"code","source":"# Handling Data Imbalances\ndef load_balanced_data(files, size, start_index):\n    half_size = int(size/2)\n    count=0\n    res = []\n    y = []\n    for file in files[start_index:]:\n        if (count!=half_size):\n            if file[-5] == '1' and file.endswith(\".png\"):\n                img = load_img(file, target_size = (50,50))\n                pixels = img_to_array(img)\n                pixels /= 255\n                res.append(pixels)\n                y.append(1)\n                count += 1\n                \n    for file in files[start_index:]:\n        if(count!=0):\n            if(file[-5] == '0'):\n                img = load_img(file, target_size = (50,50))\n                pixels = img_to_array(img)\n                pixels /= 255\n                res.append(pixels)\n                y.append(0)\n                count -= 1\n    return np.stack(res), y","metadata":{"execution":{"iopub.status.busy":"2021-08-29T20:59:00.476576Z","iopub.execute_input":"2021-08-29T20:59:00.476949Z","iopub.status.idle":"2021-08-29T20:59:00.484536Z","shell.execute_reply.started":"2021-08-29T20:59:00.476917Z","shell.execute_reply":"2021-08-29T20:59:00.483732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#15000-15000 imgs of each cls in train data\nX_train2, Y_train2 = load_balanced_data(files,30000, 0)\n#6000 imgs in test set(both cls 10000)\nX_test2, Y_test2 = load_balanced_data(files, 6000, 120000)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T20:59:10.898054Z","iopub.execute_input":"2021-08-29T20:59:10.898415Z","iopub.status.idle":"2021-08-29T20:59:41.125548Z","shell.execute_reply.started":"2021-08-29T20:59:10.898388Z","shell.execute_reply":"2021-08-29T20:59:41.124302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#visualize distribution now\nsns.countplot(Y_train2)\n#plt.figure(figsize=(6, 4))\nplt.title(\"Training data distribution\")\nplt.xlabel(\"0-IDC Negative      1-IDC Positive\")\nplt.savefig(\"Training_distri.png\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-29T20:59:46.792434Z","iopub.execute_input":"2021-08-29T20:59:46.792792Z","iopub.status.idle":"2021-08-29T20:59:47.022512Z","shell.execute_reply.started":"2021-08-29T20:59:46.792758Z","shell.execute_reply":"2021-08-29T20:59:47.021722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_train2 = to_categorical(Y_train2)\nY_test2 = to_categorical(Y_test2)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T20:59:55.475524Z","iopub.execute_input":"2021-08-29T20:59:55.475909Z","iopub.status.idle":"2021-08-29T20:59:55.510452Z","shell.execute_reply.started":"2021-08-29T20:59:55.475875Z","shell.execute_reply":"2021-08-29T20:59:55.509443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Augmentation","metadata":{}},{"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator\n\ntrain_datagen = ImageDataGenerator(rescale=1.0/255,\n                                height_shift_range=0.2,\n                                width_shift_range=0.2,\n                                horizontal_flip = True,\n                                vertical_flip = True,\n                                zoom_range=0.2,\n                                shear_range=0.2)\n\ntest_datagen = ImageDataGenerator(rescale=1.0/255)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T21:00:06.528758Z","iopub.execute_input":"2021-08-29T21:00:06.529089Z","iopub.status.idle":"2021-08-29T21:00:06.535287Z","shell.execute_reply.started":"2021-08-29T21:00:06.529059Z","shell.execute_reply":"2021-08-29T21:00:06.534281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_generator = train_datagen.flow(X_train2, Y_train2, batch_size=32)\nval_generator = test_datagen.flow(X_test2, Y_test2, batch_size=32)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T21:00:16.339231Z","iopub.execute_input":"2021-08-29T21:00:16.339544Z","iopub.status.idle":"2021-08-29T21:00:16.343875Z","shell.execute_reply.started":"2021-08-29T21:00:16.339516Z","shell.execute_reply":"2021-08-29T21:00:16.342987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tot_imgs = X_train2.shape[0]\ntot_imgs","metadata":{"execution":{"iopub.status.busy":"2021-08-29T21:00:21.139131Z","iopub.execute_input":"2021-08-29T21:00:21.139460Z","iopub.status.idle":"2021-08-29T21:00:21.145606Z","shell.execute_reply.started":"2021-08-29T21:00:21.139429Z","shell.execute_reply":"2021-08-29T21:00:21.144437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Obtaining x_train and y_train back from ImageDataGenertor\n#using simply next will only give 1 batch of data. To load complete we need to use loop\nfrom tqdm import tqdm\nbatch_size=32\ntrain_generator.reset()\nX_train, y_train = next(train_generator)\nfor i in tqdm(range(int(tot_imgs / batch_size)-1)): #1st batch is already fetched before the for loop\n    img, label = next(train_generator)\n    X_train = np.append(X_train, img, axis=0 )\n    y_train = np.append(y_train, label, axis=0)\nprint(X_train.shape, y_train.shape)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-29T21:00:34.188451Z","iopub.execute_input":"2021-08-29T21:00:34.188781Z","iopub.status.idle":"2021-08-29T21:03:37.122802Z","shell.execute_reply.started":"2021-08-29T21:00:34.188749Z","shell.execute_reply":"2021-08-29T21:03:37.120206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating a Vgg16 with Cross-validation\n\n### Architechture of Vgg 16 with data augmentation\n- Splitted the training data into 5 folds using KFold cross-validation\n- Created VGG16 model object with image input shape as 50*50*3\n- Existing trained layers, set trainable to False and added out output layers with 1024 filters.\n- compiled model using Adam optimizer, and binary crossentropy as loss function\n- finally model trained on each set with 35 epochs, and batch size as 32","metadata":{}},{"cell_type":"code","source":"# Creaing VGG16 Architecture and its Libraries\nfrom keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, BatchNormalization, Dropout, Add, ReLU, Input, Lambda\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.metrics import Recall, Precision, AUC \nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.applications.vgg16 import preprocess_input\nfrom sklearn.model_selection import StratifiedKFold, KFold","metadata":{"execution":{"iopub.status.busy":"2021-08-29T21:04:51.480221Z","iopub.execute_input":"2021-08-29T21:04:51.480530Z","iopub.status.idle":"2021-08-29T21:04:51.487967Z","shell.execute_reply.started":"2021-08-29T21:04:51.480500Z","shell.execute_reply":"2021-08-29T21:04:51.485079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score","metadata":{"execution":{"iopub.status.busy":"2021-08-29T21:05:00.661082Z","iopub.execute_input":"2021-08-29T21:05:00.661409Z","iopub.status.idle":"2021-08-29T21:05:00.667494Z","shell.execute_reply.started":"2021-08-29T21:05:00.661376Z","shell.execute_reply":"2021-08-29T21:05:00.666565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#K fold cross validation with 5 folds\nkfold = KFold(n_splits=5, shuffle=True, random_state=10)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T21:05:21.381513Z","iopub.execute_input":"2021-08-29T21:05:21.381880Z","iopub.status.idle":"2021-08-29T21:05:21.388133Z","shell.execute_reply.started":"2021-08-29T21:05:21.381847Z","shell.execute_reply":"2021-08-29T21:05:21.387297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cvscores = []\noutput_classes=2\ni=1\nfor train, test in kfold.split(X_train, y_train):\n    \n    print(f\"Fold {i}\")\n    #create vgg model\n    vgg = VGG16(input_shape=(50,50,3),weights='imagenet', include_top=False)\n\n    #we do not want to train existing weights\n    for layer in vgg.layers:\n        layer.trainable = False\n        \n    #Add layers at the end\n    x = Flatten()(vgg.output)\n    x = Dense(1024, activation='relu')(x)\n    prediction = Dense(output_classes, activation='sigmoid')(x) #last prediction layer\n    \n    # create a model object\n    model = Model(inputs=vgg.input, outputs=prediction)\n    \n    #if you want the summary of each \n    #model.summary()\n    \n    #compile the model with defining hyperparameters\n    model.compile(loss = \"binary_crossentropy\", optimizer=\"adam\", metrics=[\"acc\"])\n    \n    #train_data = tf.reshape(train, [50, 50])\n    #fit the model\n    history = model.fit(X_train[train], y_train[train], validation_data=(X_train[test], y_train[test]), epochs=35, batch_size=32)\n    \n    #evaluate the model\n    scores = model.evaluate(X_train[test], y_train[test], verbose=0)\n    \n    i+=1\n    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n    print(\"\\n\")\n    cvscores.append(scores[1] * 100)\n    \n    #Plotting for each fold accuracy\n    plt.figure(figsize=(6, 4))\n    plt.plot(history.history['acc'], label=\"train-acc\")\n    plt.plot(history.history['val_acc'], label=\"val-acc\")\n    plt.title(f\"Fold {i} analyzing accuracy\")\n    plt.legend(loc=\"best\")\n    plt.xlabel(\"Number of epochs\")\n    plt.savefig(f\"Fold_{i}_acc.png\")\n    plt.show()\n    \n    #Plotting for each fold loss\n    plt.figure(figsize=(6,4))\n    plt.plot(history.history['loss'], label=\"train-loss\")\n    plt.plot(history.history['val_loss'], label=\"val-loss\")\n    #plt.title(f\"Fold {i} analyzing loss)\n    plt.legend()\n    plt.xlabel(\"Number of epochs\")\n    plt.savefig(f\"Fold_{i}_loss.png\")\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-08-29T20:56:27.009885Z","iopub.status.idle":"2021-08-29T20:56:27.010499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.mean(cvscores)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T20:56:27.011630Z","iopub.status.idle":"2021-08-29T20:56:27.012293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def recall(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives / (possible_positives + K.epsilon())\n    return recall\n\ndef precision(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision","metadata":{"execution":{"iopub.status.busy":"2021-08-29T21:06:09.320144Z","iopub.execute_input":"2021-08-29T21:06:09.320471Z","iopub.status.idle":"2021-08-29T21:06:09.326374Z","shell.execute_reply.started":"2021-08-29T21:06:09.320441Z","shell.execute_reply":"2021-08-29T21:06:09.325534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## VGG16 Model","metadata":{}},{"cell_type":"code","source":"# VGG-16 Model\noutput_classes = 2\nvgg = VGG16(input_shape=(50,50,3),weights='imagenet', include_top=False)\n\n#we do not want to train existing weights\nfor layer in vgg.layers:\n    layer.trainable = False\n        \n#Add layers at the end\nx = Flatten()(vgg.output)\nx = Dense(1024, activation='relu')(x)\nprediction = Dense(output_classes, activation='sigmoid')(x) #last prediction layer\n    \n# create a model object\nmodel = Model(inputs=vgg.input, outputs=prediction)\n    \n    #if you want the summary of each \n    #model.summary()\n    \n#compile the model with defining hyperparameters\nmodel.compile(loss = \"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\", recall, precision])\n    \n#train_data = tf.reshape(train, [50, 50])\n#fit the model\nhistory = model.fit(X_train2, Y_train2, validation_data = (X_test2, Y_test2), epochs=20, batch_size=32)\n    ","metadata":{"execution":{"iopub.status.busy":"2021-08-29T21:06:21.050118Z","iopub.execute_input":"2021-08-29T21:06:21.050439Z","iopub.status.idle":"2021-08-29T21:09:51.466068Z","shell.execute_reply.started":"2021-08-29T21:06:21.050409Z","shell.execute_reply":"2021-08-29T21:09:51.465234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss, accuracy, recall, precision = model.evaluate(X_test2, Y_test2,verbose=1)\nprint(\"VGG16 Metrics Score\")\nprint(f\"accuracy: {accuracy} \\nprecision: {precision} \\nrecall: {recall} \\nloss: {loss}\")","metadata":{"execution":{"iopub.status.busy":"2021-08-29T21:09:59.939532Z","iopub.execute_input":"2021-08-29T21:09:59.939889Z","iopub.status.idle":"2021-08-29T21:10:01.766012Z","shell.execute_reply.started":"2021-08-29T21:09:59.939858Z","shell.execute_reply":"2021-08-29T21:10:01.765094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(6, 4))\nplt.plot(history.history['accuracy'], label=\"train-acc\")\nplt.plot(history.history['val_accuracy'], label=\"val-acc\")\nplt.title(\"Analyzing Accuracy of VGG16\")\nplt.legend(loc=\"best\")\nplt.xlabel(\"Number of epochs\")\nplt.savefig(\"VGG16_accuracy.png\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-29T21:10:17.094335Z","iopub.execute_input":"2021-08-29T21:10:17.094671Z","iopub.status.idle":"2021-08-29T21:10:17.324888Z","shell.execute_reply.started":"2021-08-29T21:10:17.094625Z","shell.execute_reply":"2021-08-29T21:10:17.324073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Loss graph\nplt.figure(figsize=(6,4))\nplt.plot(history.history['loss'], label=\"train-loss\")\nplt.plot(history.history['val_loss'], label=\"val-loss\")\nplt.title(\"Analyzing Loss for VGG16\")\nplt.legend()\nplt.xlabel(\"Number of epochs\")\nplt.savefig(\"VGG16_loss.png\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-29T21:10:29.514872Z","iopub.execute_input":"2021-08-29T21:10:29.515195Z","iopub.status.idle":"2021-08-29T21:10:29.737525Z","shell.execute_reply.started":"2021-08-29T21:10:29.515166Z","shell.execute_reply":"2021-08-29T21:10:29.736501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Resnet50 Model\n","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.applications import ResNet50","metadata":{"execution":{"iopub.status.busy":"2021-08-29T21:11:38.603303Z","iopub.execute_input":"2021-08-29T21:11:38.603633Z","iopub.status.idle":"2021-08-29T21:11:38.609430Z","shell.execute_reply.started":"2021-08-29T21:11:38.603603Z","shell.execute_reply":"2021-08-29T21:11:38.606636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#base_model = Sequential()\n#base_model.add(ResNet50(include_top=False, weights='imagenet', pooling='max'))\n#base_model.add(Dense(2, activation='sigmoid'))","metadata":{"execution":{"iopub.status.busy":"2021-08-29T20:56:27.024349Z","iopub.status.idle":"2021-08-29T20:56:27.024991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#compile model\n#base_model.compile(optimizer = tf.keras.optimizers.SGD(lr=0.0001), loss = 'binary_crossentropy', metrics = ['acc',recall, precision])\n\n#training resnet\n#resnet_history = base_model.fit(X_train2, Y_train2, validation_data = (X_test2, Y_test2), epochs = 20)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-29T20:56:27.026060Z","iopub.status.idle":"2021-08-29T20:56:27.026629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#loss, accuracy, recall, precision = base_model.evaluate(X_test2, Y_test2,verbose=1)\n#print(\"Resnet Metrics\")\n#print(f\"accuracy: {accuracy} \\nprecision: {precision} \\nrecall: {recall} \\nloss: {loss}\")","metadata":{"execution":{"iopub.status.busy":"2021-08-29T20:56:27.027747Z","iopub.status.idle":"2021-08-29T20:56:27.028337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plt.figure(figsize=(6, 4))\n#plt.plot(resnet_history.history['acc'], label=\"train-acc\")\n#plt.plot(resnet_history.history['val_acc'], label=\"val-acc\")\n#plt.title(\"Analyzing Accuracy of ResNet50\")\n#plt.legend(loc=\"best\")\n#plt.savefig(\"resnet_accuracy.png\")\n#plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-29T20:56:27.029638Z","iopub.status.idle":"2021-08-29T20:56:27.030296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Loss graph\n#plt.figure(figsize=(6,4))\n#plt.plot(resnet_history.history['loss'], label=\"train-loss\")\n#plt.plot(resnet_history.history['val_loss'], label=\"val-loss\")\n#plt.title(\"Analyzing Loss for Homogenous ResNet50\")\n#plt.legend()\n#plt.savefig(\"homo_resnet_loss.png\")\n#plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-29T20:56:27.031498Z","iopub.status.idle":"2021-08-29T20:56:27.032146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating Ensemble of Transfer Learning\n\nEnsemble\n- Define the ensemble function first\n\n- Here, we are iterating over all the models to get the last layers as output\n- Then we are adding an merge layer (average) to compute the average output scores of all the models.\n- Compile the model","metadata":{}},{"cell_type":"code","source":"# Creating an Ensemble Model\ndef create_model(img_size=50, channels=3):\n    model = Sequential()\n    model.add(ResNet50(include_top=False, weights='imagenet', pooling='max'))\n    model.add(Dense(2, activation='sigmoid'))\n    #compile model\n    model.compile(optimizer = tf.keras.optimizers.Adam(lr=0.0001), loss = 'binary_crossentropy',\n                  metrics = ['acc'])\n    return model\n\ndef create_vgg():\n    vgg = VGG16(input_shape=(50,50,3),weights='imagenet', include_top=False)\n    for layer in vgg.layers:\n        layer.trainable = False\n        \n    #Add layers at the end\n    x = Flatten()(vgg.output)\n    x = Dense(1024, activation='relu')(x)\n    prediction = Dense(output_classes, activation='sigmoid')(x) #last prediction layer\n    \n    # create a model object\n    model = Model(inputs=vgg.input, outputs=prediction)\n    model.compile(loss = \"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-08-29T21:11:48.929749Z","iopub.execute_input":"2021-08-29T21:11:48.930084Z","iopub.status.idle":"2021-08-29T21:11:48.937651Z","shell.execute_reply.started":"2021-08-29T21:11:48.930054Z","shell.execute_reply":"2021-08-29T21:11:48.936812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def ensemble(model, model_input):\n    Models_output = [model(model_input) for model in models]\n    avg = keras.layers.average(Models_output)\n    \n    modelEnsemble = Model(inputs=model_input, outputs=avg, name=\"ensemble\")\n    modelEnsemble.summary()\n    \n    modelEnsemble.compile(tf.keras.optimizers.Adam(lr=0.0001), loss=\"binary_crossentropy\", metrics=[\"acc\"])\n    return modelEnsemble","metadata":{"execution":{"iopub.status.busy":"2021-08-29T21:11:57.904437Z","iopub.execute_input":"2021-08-29T21:11:57.904768Z","iopub.status.idle":"2021-08-29T21:11:57.911261Z","shell.execute_reply.started":"2021-08-29T21:11:57.904736Z","shell.execute_reply":"2021-08-29T21:11:57.909978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ensemble of ResNet50 and ResNet50(Homogeneous Resnet)","metadata":{}},{"cell_type":"code","source":"#combining 2 ResNet model and creating its homogeneous model\nmodel_1 = create_model(channels = 3)\nmodel_2 = create_model(channels = 3)\n\nmodels = []\n\nmodel_1._name = 'model_1'\nmodels.append(model_1)\n\nmodel_2._name = 'model_2'\nmodels.append(model_2)\n\nmodel_input = Input(shape=models[0].input_shape[1:])","metadata":{"execution":{"iopub.status.busy":"2021-08-29T21:12:07.203801Z","iopub.execute_input":"2021-08-29T21:12:07.204151Z","iopub.status.idle":"2021-08-29T21:12:11.314859Z","shell.execute_reply.started":"2021-08-29T21:12:07.204120Z","shell.execute_reply":"2021-08-29T21:12:11.313924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resnet_model = ensemble(models, model_input)\nresnet_history = resnet_model.fit(X_train2, Y_train2, validation_data = (X_test2, Y_test2), epochs=20)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T21:12:28.613741Z","iopub.execute_input":"2021-08-29T21:12:28.614064Z","iopub.status.idle":"2021-08-29T21:38:43.082265Z","shell.execute_reply.started":"2021-08-29T21:12:28.614036Z","shell.execute_reply":"2021-08-29T21:38:43.081413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plotting accuracy graph for homogeneous ResNet model\nplt.figure(figsize=(6,4))\nplt.plot(resnet_history.history['acc'], label=\"train-acc\")\nplt.plot(resnet_history.history['val_acc'], label=\"val-acc\")\nplt.title(\"Analyzing accuracy of ensemble of homogeneous\")\nplt.legend(loc=\"best\")\nplt.savefig(\"resnethomo_accuracy.png\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-29T21:41:14.960414Z","iopub.execute_input":"2021-08-29T21:41:14.960758Z","iopub.status.idle":"2021-08-29T21:41:15.183537Z","shell.execute_reply.started":"2021-08-29T21:41:14.960725Z","shell.execute_reply":"2021-08-29T21:41:15.182553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Loss graph for ensemble model (resnet50 + resnet50)\nplt.figure(figsize=(6,4))\nplt.plot(resnet_history.history['loss'], label=\"train-loss\")\nplt.plot(resnet_history.history['val_loss'], label=\"val-loss\")\nplt.title(\"Analyzing loss for ensemble of homogeneous\")\nplt.legend()\nplt.savefig(\"homo_loss.png\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-29T21:41:28.102332Z","iopub.execute_input":"2021-08-29T21:41:28.102643Z","iopub.status.idle":"2021-08-29T21:41:28.314480Z","shell.execute_reply.started":"2021-08-29T21:41:28.102612Z","shell.execute_reply":"2021-08-29T21:41:28.313606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ensemble of VGG16 AND RESNET50","metadata":{}},{"cell_type":"code","source":"#combining 2 ResNet model and creating its heterogeneous model\nmodel_1 = create_model(channels = 3)\nmodel_2 = create_vgg()\n\nmodels = []\n\nmodel_1._name = 'model_1'\nmodels.append(model_1)\n\nmodel_2._name = 'model_2'\nmodels.append(model_2)\n\nmodel_input = Input(shape=models[0].input_shape[1:])","metadata":{"execution":{"iopub.status.busy":"2021-08-29T21:41:41.948067Z","iopub.execute_input":"2021-08-29T21:41:41.948387Z","iopub.status.idle":"2021-08-29T21:41:43.929343Z","shell.execute_reply.started":"2021-08-29T21:41:41.948356Z","shell.execute_reply":"2021-08-29T21:41:43.928492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ensemble_model = ensemble(models, model_input)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T21:41:53.072293Z","iopub.execute_input":"2021-08-29T21:41:53.072604Z","iopub.status.idle":"2021-08-29T21:41:53.528701Z","shell.execute_reply.started":"2021-08-29T21:41:53.072574Z","shell.execute_reply":"2021-08-29T21:41:53.527191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ensemble_history = ensemble_model.fit(X_train2, Y_train2, validation_data = (X_test2, Y_test2), epochs=20)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T21:42:02.205347Z","iopub.execute_input":"2021-08-29T21:42:02.205661Z","iopub.status.idle":"2021-08-29T21:57:52.409596Z","shell.execute_reply.started":"2021-08-29T21:42:02.205630Z","shell.execute_reply":"2021-08-29T21:57:52.408707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plotting accuracy graph for heterogeneous model (resnet50 + vgg16)\nplt.figure(figsize=(6,4))\nplt.plot(ensemble_history.history['acc'], label=\"train-acc\")\nplt.plot(ensemble_history.history['val_acc'], label=\"val-acc\")\nplt.title(\"Analyzing Accuracy for ensemble of heterogeneous\")\nplt.legend(loc=\"best\")\nplt.xlabel(\"Number of epochs\")\nplt.savefig(\"hetero_ensemble_accuracy.png\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-29T22:01:04.292260Z","iopub.execute_input":"2021-08-29T22:01:04.292591Z","iopub.status.idle":"2021-08-29T22:01:04.520415Z","shell.execute_reply.started":"2021-08-29T22:01:04.292560Z","shell.execute_reply":"2021-08-29T22:01:04.519629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Loss graph for ensemble model\nplt.figure(figsize=(6,4))\nplt.plot(ensemble_history.history['loss'], label=\"train-loss\")\nplt.plot(ensemble_history.history['val_loss'], label=\"val-loss\")\nplt.title(\"Analyzing loss for ensemble of heterogeneous\")\nplt.legend()\nplt.xlabel(\"Number of epochs\")\nplt.savefig(\"hetero_ensemble_loss.png\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-29T22:01:28.361319Z","iopub.execute_input":"2021-08-29T22:01:28.361702Z","iopub.status.idle":"2021-08-29T22:01:28.614188Z","shell.execute_reply.started":"2021-08-29T22:01:28.361629Z","shell.execute_reply":"2021-08-29T22:01:28.613405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}